"""
æµ‹è¯• LLM æ¨¡å—
ä½¿ç”¨Qwen2.5æ¨¡å‹è¿›è¡ŒRAGç­”æ¡ˆç”Ÿæˆ
"""
import os
import sys

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from app.llm import create_llm
from app.chunker import Chunk, ChunkMetadata


def find_local_model(model_name: str):
    """
    æŸ¥æ‰¾æœ¬åœ°ç¼“å­˜çš„æ¨¡å‹è·¯å¾„
    
    Args:
        model_name: æ¨¡å‹åç§°ï¼ˆå¦‚ "Qwen/Qwen2.5-7B-Instruct"ï¼‰
    
    Returns:
        æœ¬åœ°æ¨¡å‹è·¯å¾„æˆ–None
    """
    cache_dir = os.path.join(
        os.path.expanduser("~"),
        ".cache",
        "huggingface",
        "hub"
    )
    
    # å°†æ¨¡å‹åç§°è½¬æ¢ä¸ºç›®å½•å
    model_dir_name = model_name.replace("/", "--")
    model_path = os.path.join(cache_dir, f"models--{model_dir_name}", "snapshots")
    
    if os.path.exists(model_path):
        snapshots = [d for d in os.listdir(model_path) 
                    if os.path.isdir(os.path.join(model_path, d))]
        if snapshots:
            latest_snapshot = sorted(snapshots)[-1]
            return os.path.join(model_path, latest_snapshot)
    
    return None


def create_test_chunks():
    """åˆ›å»ºæµ‹è¯•ç”¨çš„chunksæ•°æ®"""
    chunks = [
        Chunk(
            chunk_id="test_1",
            text="æ„å¤–ä¼¤å®³ä¿é™©ç†èµ”æµç¨‹è¯´æ˜ã€‚å½“å‘ç”Ÿæ„å¤–ä¼¤å®³äº‹æ•…æ—¶ï¼Œè¢«ä¿é™©äººéœ€è¦å‡†å¤‡ä»¥ä¸‹ææ–™ï¼š1. èº«ä»½è¯åŸä»¶åŠå¤å°ä»¶ï¼›2. åŒ»ç–—è¯Šæ–­è¯æ˜ä¹¦ï¼›3. åŒ»ç–—è´¹ç”¨å‘ç¥¨ï¼›4. äº‹æ•…è¯æ˜æ–‡ä»¶ï¼›5. ä¿é™©åˆåŒåŸä»¶ã€‚ç†èµ”ç”³è¯·æäº¤åï¼Œä¿é™©å…¬å¸ä¼šåœ¨15ä¸ªå·¥ä½œæ—¥å†…å®Œæˆå®¡æ ¸å¹¶æ”¯ä»˜ç†èµ”æ¬¾ã€‚",
            metadata=ChunkMetadata(
                chunk_id="test_1",
                chunk_type="paragraph",
                section_path=["ä¿é™©ç†èµ”", "æ„å¤–ä¼¤å®³ä¿é™©"],
                heading_level=2,
                char_count=150,
                image_refs=[],
                source_file="test.md"
            )
        ),
        Chunk(
            chunk_id="test_2",
            text="é‡å¤§ç–¾ç—…ä¿é™©æ¡æ¬¾è¯¦è§£ã€‚æœ¬ä¿é™©æ¶µç›–30ç§é‡å¤§ç–¾ç—…ï¼ŒåŒ…æ‹¬ï¼šæ¶æ€§è‚¿ç˜¤ã€æ€¥æ€§å¿ƒè‚Œæ¢—å¡ã€è„‘ä¸­é£åé—ç—‡ã€é‡å¤§å™¨å®˜ç§»æ¤æœ¯æˆ–é€ è¡€å¹²ç»†èƒç§»æ¤æœ¯ã€å† çŠ¶åŠ¨è„‰æ­æ¡¥æœ¯ã€ç»ˆæœ«æœŸè‚¾ç—…ç­‰ã€‚ä¿é™©é‡‘é¢æ ¹æ®æŠ•ä¿æ—¶çº¦å®šçš„ä¿é¢ç¡®å®šï¼Œç­‰å¾…æœŸä¸º90å¤©ã€‚",
            metadata=ChunkMetadata(
                chunk_id="test_2",
                chunk_type="paragraph",
                section_path=["ä¿é™©æ¡æ¬¾", "é‡å¤§ç–¾ç—…ä¿é™©"],
                heading_level=2,
                char_count=120,
                image_refs=[],
                source_file="test.md"
            )
        ),
        Chunk(
            chunk_id="test_3",
            text="è½¦é™©ç†èµ”æ‰€éœ€ææ–™æ¸…å•ã€‚å‘ç”Ÿäº¤é€šäº‹æ•…åï¼Œç”³è¯·è½¦é™©ç†èµ”éœ€è¦å‡†å¤‡ï¼šé©¾é©¶è¯ã€è¡Œé©¶è¯ã€èº«ä»½è¯ã€äº¤é€šäº‹æ•…è´£ä»»è®¤å®šä¹¦ã€è½¦è¾†ç»´ä¿®å‘ç¥¨ã€åŒ»ç–—è´¹ç”¨å‘ç¥¨ï¼ˆå¦‚æœ‰äººå‘˜å—ä¼¤ï¼‰ã€äº‹æ•…ç°åœºç…§ç‰‡ã€ä¿é™©å•åŸä»¶ã€‚æäº¤å®Œæ•´ææ–™åï¼Œä¿é™©å…¬å¸ä¼šåœ¨10ä¸ªå·¥ä½œæ—¥å†…å®Œæˆç†èµ”å®¡æ ¸ã€‚",
            metadata=ChunkMetadata(
                chunk_id="test_3",
                chunk_type="paragraph",
                section_path=["ä¿é™©ç†èµ”", "è½¦é™©"],
                heading_level=2,
                char_count=130,
                image_refs=[],
                source_file="test.md"
            )
        )
    ]
    return chunks


def main():
    print("=" * 70)
    print("æµ‹è¯• LLM æ¨¡å—")
    print("=" * 70)
    print("\nåŠŸèƒ½æµ‹è¯•ï¼š")
    print("  1. Qwen2.5æ¨¡å‹åŠ è½½")
    print("  2. RAG Promptæ„å»º")
    print("  3. ç­”æ¡ˆç”Ÿæˆ")
    print("  4. æ¨¡å‹ç¼“å­˜æœºåˆ¶")
    print("=" * 70)
    
    try:
        # 1. åŠ è½½LLMæ¨¡å‹
        print("\nğŸ“¦ æ­¥éª¤1: åŠ è½½LLMæ¨¡å‹...")
        print("âš  æ³¨æ„: é¦–æ¬¡è¿è¡Œéœ€è¦ä¸‹è½½Qwen2.5æ¨¡å‹ï¼ˆçº¦6GBï¼‰ï¼Œè¯·è€å¿ƒç­‰å¾…...")
        print("  âœ“ å·²é…ç½®ä½¿ç”¨å›½å†…é•œåƒæºåŠ é€Ÿä¸‹è½½")
        print("  ğŸ’¡ æç¤º: å¦‚æœæ˜¾å­˜ä¸è¶³ï¼Œå¯ä»¥ä½¿ç”¨Qwen2.5-1.5B-Instructï¼ˆçº¦3GBï¼‰")
        
        model_name = "Qwen/Qwen2.5-3B-Instruct"
        model_path = find_local_model(model_name)
        
        if model_path:
            print(f"âœ“ æ‰¾åˆ°æœ¬åœ°ç¼“å­˜æ¨¡å‹: {model_path}")
            llm = create_llm(model_path=model_path, use_mirror=False)
        else:
            print("âš  æœªæ‰¾åˆ°æœ¬åœ°æ¨¡å‹ï¼Œå°†ä»ç½‘ç»œä¸‹è½½...")
            llm = create_llm(model_name=model_name, use_mirror=True)
        
        print("âœ“ LLMåŠ è½½æˆåŠŸ")
        
        # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
        model_info = llm.get_model_info()
        print("\næ¨¡å‹è¯¦ç»†ä¿¡æ¯:")
        for key, value in model_info.items():
            print(f"  â€¢ {key}: {value}")
        
        # 2. åˆ›å»ºæµ‹è¯•chunks
        print("\nğŸ“¦ æ­¥éª¤2: åˆ›å»ºæµ‹è¯•chunksæ•°æ®...")
        chunks = create_test_chunks()
        print(f"âœ“ æˆåŠŸåˆ›å»º {len(chunks)} ä¸ªæµ‹è¯•chunks")
        
        # 3. æµ‹è¯•æŸ¥è¯¢
        print("\n" + "=" * 70)
        print("ğŸ” æµ‹è¯•RAGç­”æ¡ˆç”Ÿæˆ")
        print("=" * 70)
        
        test_queries = [
            "å¦‚ä½•ç”³è¯·æ„å¤–é™©ç†èµ”ï¼Ÿéœ€è¦å‡†å¤‡å“ªäº›ææ–™ï¼Ÿ",
            "é‡ç–¾é™©åŒ…å«å“ªäº›ç–¾ç—…ï¼Ÿ",
            "è½¦é™©ç†èµ”éœ€è¦å‡†å¤‡ä»€ä¹ˆææ–™ï¼Ÿ"
        ]
        
        for query_idx, query in enumerate(test_queries, 1):
            print(f"\n{'='*70}")
            print(f"æŸ¥è¯¢ {query_idx}: {query}")
            print("=" * 70)
            
            # ç”Ÿæˆç­”æ¡ˆ
            result = llm.answer(query, chunks)
            
            print(f"\nâœ“ ç”Ÿæˆçš„ç­”æ¡ˆ:")
            print(f"  {result['answer']}")
            print(f"\n  å…ƒä¿¡æ¯:")
            print(f"    - ä½¿ç”¨çš„chunksæ•°é‡: {result['num_chunks_used']}")
            print(f"    - Prompté•¿åº¦: {result['prompt_length']} å­—ç¬¦")
        
        # 4. æµ‹è¯•æ¨¡å‹ç¼“å­˜æœºåˆ¶
        print("\n" + "=" * 70)
        print("ğŸ’¾ æµ‹è¯•æ¨¡å‹ç¼“å­˜æœºåˆ¶ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰")
        print("=" * 70)
        
        from app.llm import get_model_cache_info, clear_model_cache
        
        # æ˜¾ç¤ºç¼“å­˜ä¿¡æ¯
        cache_info = get_model_cache_info()
        print(f"\nå½“å‰æ¨¡å‹ç¼“å­˜çŠ¶æ€:")
        print(f"  ç¼“å­˜æ•°é‡: {cache_info['cache_count']}")
        print(f"  ç¼“å­˜çš„æ¨¡å‹: {cache_info['cached_models']}")
        
        # åˆ›å»ºç¬¬äºŒä¸ªå®ä¾‹ï¼Œåº”è¯¥å¤ç”¨æ¨¡å‹
        print("\nåˆ›å»ºç¬¬äºŒä¸ªLLMå®ä¾‹ï¼ˆåº”è¯¥å¤ç”¨æ¨¡å‹ï¼Œä¸å ç”¨é¢å¤–æ˜¾å­˜ï¼‰...")
        llm2 = create_llm(model_name=model_name, use_mirror=True)
        print("âœ“ ç¬¬äºŒä¸ªå®ä¾‹åˆ›å»ºæˆåŠŸï¼ˆå¦‚æœçœ‹åˆ°'å¤ç”¨å·²ç¼“å­˜çš„LLMæ¨¡å‹'ï¼Œè¯´æ˜ç¼“å­˜ç”Ÿæ•ˆï¼‰")
        
        # å†æ¬¡æŸ¥çœ‹ç¼“å­˜ä¿¡æ¯
        cache_info2 = get_model_cache_info()
        print(f"\nç¼“å­˜çŠ¶æ€ï¼ˆåˆ›å»ºç¬¬äºŒä¸ªå®ä¾‹åï¼‰:")
        print(f"  ç¼“å­˜æ•°é‡: {cache_info2['cache_count']}")
        print(f"  è¯´æ˜: ä¸¤ä¸ªå®ä¾‹å…±äº«åŒä¸€ä¸ªæ¨¡å‹ï¼ŒèŠ‚çœæ˜¾å­˜ï¼")
        
        # 5. æµ‹è¯•ä¸åŒçš„ç”Ÿæˆå‚æ•°
        print("\n" + "=" * 70)
        print("âš™ï¸ æµ‹è¯•ä¸åŒçš„ç”Ÿæˆå‚æ•°")
        print("=" * 70)
        
        query = "æ„å¤–é™©ç†èµ”éœ€è¦å¤šé•¿æ—¶é—´ï¼Ÿ"
        print(f"\næŸ¥è¯¢: {query}")
        
        # é»˜è®¤å‚æ•°
        result1 = llm.answer(query, chunks[:1])
        print(f"\nã€é»˜è®¤å‚æ•°ã€‘ç­”æ¡ˆ: {result1['answer'][:200]}...")
        
        # è°ƒæ•´temperatureï¼ˆæ›´ä¿å®ˆï¼‰
        result2 = llm.answer(query, chunks[:1], temperature=0.3)
        print(f"\nã€Temperature=0.3ã€‘ç­”æ¡ˆ: {result2['answer'][:200]}...")
        
        print("\n" + "=" * 70)
        print("âœ“ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        print("=" * 70)
        print("\næç¤ºï¼š")
        print("  â€¢ LLMæ¨¡å‹å·²æˆåŠŸåŠ è½½å¹¶æµ‹è¯•")
        print("  â€¢ æ¨¡å‹ç¼“å­˜æœºåˆ¶ï¼šå¤šä¸ªå®ä¾‹å…±äº«æ¨¡å‹ï¼ŒèŠ‚çœæ˜¾å­˜")
        print("  â€¢ å¯ä»¥é€šè¿‡è°ƒæ•´temperatureå’Œtop_på‚æ•°æ§åˆ¶ç”Ÿæˆæ•ˆæœ")
        print("  â€¢ å¦‚éœ€é‡Šæ”¾æ˜¾å­˜ï¼Œå¯è°ƒç”¨: from app.llm import clear_model_cache; clear_model_cache()")
        
        return True
        
    except Exception as e:
        print(f"\nâœ— é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        print("\næç¤ºï¼š")
        print("  â€¢ ç¡®ä¿å·²å®‰è£…transformers: pip install transformers")
        print("  â€¢ Qwen2.5-3Béœ€è¦çº¦6-8GBæ˜¾å­˜ï¼Œå¦‚æœæ˜¾å­˜ä¸è¶³ï¼š")
        print("    - ä½¿ç”¨Qwen2.5-1.5B-Instructï¼ˆçº¦3-4GBæ˜¾å­˜ï¼‰")
        print("    - æˆ–ä½¿ç”¨CPUæ¨¡å¼ï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")
        print("  â€¢ å¦‚æœæ¨¡å‹æœªä¸‹è½½ï¼Œé¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½ï¼ˆçº¦6GBï¼‰")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
