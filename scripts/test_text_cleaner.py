
"""
æµ‹è¯•æ–‡æœ¬æ¸…æ´—åŠŸèƒ½çš„è„šæœ¬
"""

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from app.text_cleaner import TextCleaner
from app.chunker import SemanticChunker
import json


def test_basic_cleaning():
    """æµ‹è¯•åŸºç¡€æ¸…æ´—åŠŸèƒ½"""
    print("=" * 80)
    print("æµ‹è¯•1: åŸºç¡€æ¸…æ´—åŠŸèƒ½")
    print("=" * 80)
    
    cleaner = TextCleaner()
    
    # æµ‹è¯•æ–‡æœ¬ï¼ˆåŒ…å«é¡µç ã€é¡µçœ‰é¡µè„šã€OCRæ–­å¥ï¼‰
    test_text = """ç¬¬1é¡µ
è¿™æ˜¯é¡µçœ‰å†…å®¹ è¿™æ˜¯é¡µçœ‰å†…å®¹
# ç¬¬ä¸€ç«  ä¿é™©è´£ä»»

æœ¬ä¿é™©åˆåŒçº¦å®šçš„ä¿é™©è´£ä»»åŒ…æ‹¬ä»¥ä¸‹å†…å®¹ã€‚è¢«ä¿é™©äººå› æ„å¤–ä¼¤å®³å¯¼è‡´èº«æ•…çš„ï¼Œä¿é™©äººæŒ‰ç…§åˆåŒçº¦å®šç»™ä»˜ä¿é™©é‡‘ã€‚è¢«ä¿é™©äººå› ç–¾ç—…å¯¼è‡´èº«æ•…çš„ï¼Œä¿é™©äººæŒ‰ç…§åˆåŒçº¦å®šç»™ä»˜ä¿é™©é‡‘ã€‚

è¢«ä¿é™©äººå› æ„å¤–ä¼¤å®³å¯¼è‡´ä¼¤æ®‹çš„ï¼Œä¿é™©äººæŒ‰ç…§åˆåŒçº¦å®šç»™ä»˜ä¿é™©é‡‘ã€‚è¢«ä¿é™©äººå› ç–¾ç—…å¯¼è‡´ä¼¤æ®‹çš„ï¼Œä¿é™©äººæŒ‰ç…§åˆåŒçº¦å®šç»™ä»˜ä¿é™©é‡‘ã€‚

ç¬¬2é¡µ
è¿™æ˜¯é¡µçœ‰å†…å®¹ è¿™æ˜¯é¡µçœ‰å†…å®¹
# ç¬¬äºŒç«  ä¿é™©é‡‘ç”³è¯·

ä¿é™©é‡‘ç”³è¯·äººåº”å‘ä¿é™©äººæäº¤ä»¥ä¸‹ææ–™ã€‚ä¿é™©é‡‘ç”³è¯·äººå› ç‰¹æ®ŠåŸå› ä¸èƒ½æä¾›ä»¥ä¸‹ææ–™çš„ï¼Œåº”æä¾›å…¶ä»–åˆæ³•æœ‰æ•ˆçš„ææ–™ã€‚

ç¬¬3é¡µ
è¿™æ˜¯é¡µçœ‰å†…å®¹ è¿™æ˜¯é¡µçœ‰å†…å®¹
"""
    
    print("åŸå§‹æ–‡æœ¬:")
    print(test_text)
    print("\n" + "-" * 80 + "\n")
    
    cleaned = cleaner.basic_clean(test_text)
    
    print("æ¸…æ´—åæ–‡æœ¬:")
    print(cleaned)
    print("\n" + "-" * 80 + "\n")
    
    return cleaned


def test_sentence_splitting():
    """æµ‹è¯•å¥çº§æ‹†åˆ†"""
    print("=" * 80)
    print("æµ‹è¯•2: å¥çº§æ‹†åˆ†")
    print("=" * 80)
    
    cleaner = TextCleaner()
    
    test_text = """æœ¬ä¿é™©åˆåŒçº¦å®šçš„ä¿é™©è´£ä»»åŒ…æ‹¬ä»¥ä¸‹å†…å®¹ã€‚è¢«ä¿é™©äººå› æ„å¤–ä¼¤å®³å¯¼è‡´èº«æ•…çš„ï¼Œä¿é™©äººæŒ‰ç…§åˆåŒçº¦å®šç»™ä»˜ä¿é™©é‡‘ï¼
è¢«ä¿é™©äººå› ç–¾ç—…å¯¼è‡´èº«æ•…çš„ï¼Œä¿é™©äººæŒ‰ç…§åˆåŒçº¦å®šç»™ä»˜ä¿é™©é‡‘ï¼Ÿ
ä¿é™©é‡‘ç”³è¯·äººåº”å‘ä¿é™©äººæäº¤ä»¥ä¸‹ææ–™ï¼›ä¿é™©é‡‘ç”³è¯·äººå› ç‰¹æ®ŠåŸå› ä¸èƒ½æä¾›ä»¥ä¸‹ææ–™çš„ï¼Œåº”æä¾›å…¶ä»–åˆæ³•æœ‰æ•ˆçš„ææ–™ã€‚"""
    
    print("åŸå§‹æ–‡æœ¬:")
    print(test_text)
    print("\n" + "-" * 80 + "\n")
    
    sentences = cleaner.split_into_sentences(test_text)
    
    print(f"æ‹†åˆ†æˆ {len(sentences)} ä¸ªå¥å­:")
    for i, sentence in enumerate(sentences, 1):
        print(f"{i}. {sentence}")
    
    return sentences


def test_boilerplate_detection():
    """æµ‹è¯•å…œåº•è¯æœ¯è¯†åˆ«"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•3: å…œåº•è¯æœ¯è¯†åˆ«")
    print("=" * 80)
    
    cleaner = TextCleaner()
    
    test_sentences = [
        "æœ¬åˆåŒæœªå°½äº‹å®œï¼ŒæŒ‰ç…§ç›¸å…³æ³•å¾‹æ³•è§„æ‰§è¡Œã€‚",
        "ä¿é™©äººä¿ç•™æœ€ç»ˆè§£é‡Šæƒã€‚",
        "æœ¬åˆåŒçš„è§£é‡Šæƒå½’ä¿é™©äººæ‰€æœ‰ã€‚",
        "è¢«ä¿é™©äººå› æ„å¤–ä¼¤å®³å¯¼è‡´èº«æ•…çš„ï¼Œä¿é™©äººæŒ‰ç…§åˆåŒçº¦å®šç»™ä»˜ä¿é™©é‡‘ã€‚",
        "å…¶ä»–æœªå°½äº‹å®œï¼ŒæŒ‰ç…§åŒæ–¹çº¦å®šæ‰§è¡Œã€‚",
    ]
    
    print("æµ‹è¯•å¥å­:")
    for i, sentence in enumerate(test_sentences, 1):
        is_boilerplate = cleaner.is_boilerplate_sentence(sentence)
        print(f"{i}. [{is_boilerplate}] {sentence}")
    
    return test_sentences


def test_semantic_denoise():
    """æµ‹è¯•è¯­ä¹‰é™å™ª"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•4: è¯­ä¹‰é™å™ª")
    print("=" * 80)
    
    cleaner = TextCleaner(repeat_threshold=2)
    
    test_sentences = [
        "æœ¬åˆåŒæœªå°½äº‹å®œï¼ŒæŒ‰ç…§ç›¸å…³æ³•å¾‹æ³•è§„æ‰§è¡Œã€‚",  # å…œåº•è¯æœ¯
        "è¢«ä¿é™©äººå› æ„å¤–ä¼¤å®³å¯¼è‡´èº«æ•…çš„ï¼Œä¿é™©äººæŒ‰ç…§åˆåŒçº¦å®šç»™ä»˜ä¿é™©é‡‘ã€‚",  # æ­£å¸¸å†…å®¹
        "è¢«ä¿é™©äººå› æ„å¤–ä¼¤å®³å¯¼è‡´èº«æ•…çš„ï¼Œä¿é™©äººæŒ‰ç…§åˆåŒçº¦å®šç»™ä»˜ä¿é™©é‡‘ã€‚",  # é‡å¤
        "è¢«ä¿é™©äººå› æ„å¤–ä¼¤å®³å¯¼è‡´èº«æ•…çš„ï¼Œä¿é™©äººæŒ‰ç…§åˆåŒçº¦å®šç»™ä»˜ä¿é™©é‡‘ã€‚",  # é‡å¤
        "ä¿é™©é‡‘ç”³è¯·äººåº”å‘ä¿é™©äººæäº¤ä»¥ä¸‹ææ–™ã€‚",  # æ­£å¸¸å†…å®¹
        "ä¿é™©äººä¿ç•™æœ€ç»ˆè§£é‡Šæƒã€‚",  # å…œåº•è¯æœ¯
    ]
    
    print("æµ‹è¯•å¥å­:")
    for i, sentence in enumerate(test_sentences, 1):
        print(f"{i}. {sentence}")
    
    print("\nè¯­ä¹‰é™å™ªç»“æœ:")
    sentence_infos = cleaner.semantic_denoise(test_sentences)
    
    for i, info in enumerate(sentence_infos, 1):
        status = "âŒ è·³è¿‡embedding" if info.skip_embedding else "âœ… æ­£å¸¸"
        reason = f" ({info.reason})" if info.skip_embedding else ""
        print(f"{i}. [{status}{reason}] {info.text}")
    
    return sentence_infos


def test_chunker_integration():
    """æµ‹è¯•chunkeré›†æˆ"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•5: Chunkeré›†æˆæµ‹è¯•")
    print("=" * 80)
    
    # åˆ›å»ºä¸€ä¸ªæµ‹è¯•markdownæ–‡ä»¶
    test_file = project_root / "data/test_cleaner.md"
    
    test_content = """# ä¿é™©è´£ä»»

æœ¬ä¿é™©åˆåŒçº¦å®šçš„ä¿é™©è´£ä»»åŒ…æ‹¬ä»¥ä¸‹å†…å®¹ã€‚è¢«ä¿é™©äººå› æ„å¤–ä¼¤å®³å¯¼è‡´èº«æ•…çš„ï¼Œä¿é™©äººæŒ‰ç…§åˆåŒçº¦å®šç»™ä»˜ä¿é™©é‡‘ã€‚

è¢«ä¿é™©äººå› ç–¾ç—…å¯¼è‡´èº«æ•…çš„ï¼Œä¿é™©äººæŒ‰ç…§åˆåŒçº¦å®šç»™ä»˜ä¿é™©é‡‘ã€‚

æœ¬åˆåŒæœªå°½äº‹å®œï¼ŒæŒ‰ç…§ç›¸å…³æ³•å¾‹æ³•è§„æ‰§è¡Œã€‚

ä¿é™©äººä¿ç•™æœ€ç»ˆè§£é‡Šæƒã€‚

# ä¿é™©é‡‘ç”³è¯·

ä¿é™©é‡‘ç”³è¯·äººåº”å‘ä¿é™©äººæäº¤ä»¥ä¸‹ææ–™ã€‚ä¿é™©é‡‘ç”³è¯·äººå› ç‰¹æ®ŠåŸå› ä¸èƒ½æä¾›ä»¥ä¸‹ææ–™çš„ï¼Œåº”æä¾›å…¶ä»–åˆæ³•æœ‰æ•ˆçš„ææ–™ã€‚

ä¿é™©é‡‘ç”³è¯·äººåº”å‘ä¿é™©äººæäº¤ä»¥ä¸‹ææ–™ã€‚ä¿é™©é‡‘ç”³è¯·äººå› ç‰¹æ®ŠåŸå› ä¸èƒ½æä¾›ä»¥ä¸‹ææ–™çš„ï¼Œåº”æä¾›å…¶ä»–åˆæ³•æœ‰æ•ˆçš„ææ–™ã€‚

ä¿é™©é‡‘ç”³è¯·äººåº”å‘ä¿é™©äººæäº¤ä»¥ä¸‹ææ–™ã€‚ä¿é™©é‡‘ç”³è¯·äººå› ç‰¹æ®ŠåŸå› ä¸èƒ½æä¾›ä»¥ä¸‹ææ–™çš„ï¼Œåº”æä¾›å…¶ä»–åˆæ³•æœ‰æ•ˆçš„ææ–™ã€‚
"""
    
    test_file.parent.mkdir(parents=True, exist_ok=True)
    with open(test_file, 'w', encoding='utf-8') as f:
        f.write(test_content)
    
    print(f"åˆ›å»ºæµ‹è¯•æ–‡ä»¶: {test_file}\n")
    
    # ä½¿ç”¨chunkerå¤„ç†
    chunker = SemanticChunker(
        target_chunk_size=500,
        max_chunk_size=1000,
        min_chunk_size=100,
        enable_text_cleaning=True
    )
    
    chunks = chunker.chunk_markdown_file(test_file)
    
    print(f"ç”Ÿæˆäº† {len(chunks)} ä¸ªchunks\n")
    
    for i, chunk in enumerate(chunks, 1):
        print(f"--- Chunk {i} ---")
        print(f"ID: {chunk.chunk_id}")
        print(f"ç« èŠ‚: {' > '.join(chunk.metadata.section_path)}")
        print(f"åŸå§‹æ–‡æœ¬é•¿åº¦: {len(chunk.text)} å­—ç¬¦")
        print(f"è·³è¿‡embedding: {chunk.metadata.skip_embedding}")
        
        embedding_text = chunk.get_embedding_text()
        print(f"Embeddingæ–‡æœ¬é•¿åº¦: {len(embedding_text)} å­—ç¬¦")
        
        if chunk.sentence_infos:
            skipped_count = sum(1 for info in chunk.sentence_infos if info.skip_embedding)
            print(f"è·³è¿‡embeddingçš„å¥å­æ•°: {skipped_count}/{len(chunk.sentence_infos)}")
            
            print("\nå¥å­è¯¦æƒ…:")
            for j, info in enumerate(chunk.sentence_infos, 1):
                status = "âŒ" if info.skip_embedding else "âœ…"
                reason = f" ({info.reason})" if info.skip_embedding else ""
                print(f"  {status} {j}. {info.text[:50]}...{reason}")
        
        print(f"\nåŸå§‹æ–‡æœ¬é¢„è§ˆ:\n{chunk.text[:200]}...")
        print(f"\nEmbeddingæ–‡æœ¬é¢„è§ˆ:\n{embedding_text[:200]}...")
        print()
    
    # æ¸…ç†æµ‹è¯•æ–‡ä»¶
    test_file.unlink()
    
    return chunks


def test_real_file():
    """æµ‹è¯•çœŸå®æ–‡ä»¶"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•6: çœŸå®æ–‡ä»¶æµ‹è¯•")
    print("=" * 80)
    
    test_file = project_root / "data/processed/ä¿é™©å›¾ç‰‡/ä¿é™©å›¾ç‰‡.md"
    
    if not test_file.exists():
        print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
        return None
    
    print(f"å¤„ç†æ–‡ä»¶: {test_file.name}\n")
    
    chunker = SemanticChunker(
        target_chunk_size=800,
        max_chunk_size=1500,
        min_chunk_size=200,
        enable_text_cleaning=True
    )
    
    chunks = chunker.chunk_markdown_file(test_file)
    
    print(f"âœ… æˆåŠŸç”Ÿæˆ {len(chunks)} ä¸ªchunks\n")
    
    # ç»Ÿè®¡è·³è¿‡embeddingçš„æƒ…å†µ
    total_chunks = len(chunks)
    skipped_chunks = sum(1 for c in chunks if c.metadata.skip_embedding)
    chunks_with_skipped_sentences = sum(
        1 for c in chunks 
        if c.sentence_infos and any(info.skip_embedding for info in c.sentence_infos)
    )
    
    total_sentences = sum(
        len(c.sentence_infos) if c.sentence_infos else 0 
        for c in chunks
    )
    skipped_sentences = sum(
        sum(1 for info in c.sentence_infos if info.skip_embedding)
        for c in chunks if c.sentence_infos
    )
    
    print("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  æ€»chunkæ•°: {total_chunks}")
    print(f"  å®Œå…¨è·³è¿‡embeddingçš„chunkæ•°: {skipped_chunks}")
    print(f"  åŒ…å«è·³è¿‡embeddingå¥å­çš„chunkæ•°: {chunks_with_skipped_sentences}")
    print(f"  æ€»å¥å­æ•°: {total_sentences}")
    print(f"  è·³è¿‡embeddingçš„å¥å­æ•°: {skipped_sentences}")
    
    # æ˜¾ç¤ºå‰3ä¸ªchunkçš„ç¤ºä¾‹
    print("\nğŸ“ å‰3ä¸ªchunkç¤ºä¾‹:")
    for i, chunk in enumerate(chunks[:3], 1):
        print(f"\n--- Chunk {i} ---")
        print(f"ç« èŠ‚: {' > '.join(chunk.metadata.section_path)}")
        print(f"åŸå§‹æ–‡æœ¬é•¿åº¦: {len(chunk.text)} å­—ç¬¦")
        embedding_text = chunk.get_embedding_text()
        print(f"Embeddingæ–‡æœ¬é•¿åº¦: {len(embedding_text)} å­—ç¬¦")
        
        if chunk.sentence_infos:
            skipped = [j for j, info in enumerate(chunk.sentence_infos) if info.skip_embedding]
            if skipped:
                print(f"è·³è¿‡embeddingçš„å¥å­ç´¢å¼•: {skipped}")
    
    return chunks


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("\nğŸš€ å¼€å§‹æµ‹è¯•æ–‡æœ¬æ¸…æ´—åŠŸèƒ½\n")
    
    try:
        # æµ‹è¯•1: åŸºç¡€æ¸…æ´—
        cleaned = test_basic_cleaning()
        
        # æµ‹è¯•2: å¥çº§æ‹†åˆ†
        sentences = test_sentence_splitting()
        
        # æµ‹è¯•3: å…œåº•è¯æœ¯è¯†åˆ«
        test_boilerplate_detection()
        
        # æµ‹è¯•4: è¯­ä¹‰é™å™ª
        sentence_infos = test_semantic_denoise()
        
        # æµ‹è¯•5: Chunkeré›†æˆ
        chunks = test_chunker_integration()
        
        # æµ‹è¯•6: çœŸå®æ–‡ä»¶
        real_chunks = test_real_file()
        
        print("\n" + "=" * 80)
        print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
        print("=" * 80)
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
