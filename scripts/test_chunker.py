#!/usr/bin/env python3
# scripts/test_chunker.py

"""
æµ‹è¯•chunkeråŠŸèƒ½çš„è„šæœ¬
"""

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from app.chunker import SemanticChunker, SemanticSplitter, InsuranceTerminology
import json


def test_single_file():
    """æµ‹è¯•å•ä¸ªæ–‡ä»¶çš„åˆ†å—"""
    print("=" * 80)
    print("æµ‹è¯•1: å•ä¸ªæ–‡ä»¶åˆ†å—")
    print("=" * 80)
    
    # æµ‹è¯•æ–‡ä»¶è·¯å¾„
    test_file = project_root / "data/processed/ä¿é™©åŸºç¡€çŸ¥å¤šå°‘/ä¿é™©åŸºç¡€çŸ¥å¤šå°‘/hybrid_auto/ä¿é™©åŸºç¡€çŸ¥å¤šå°‘.md"
    
    if not test_file.exists():
        print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
        return
    
    print(f"ğŸ“„ å¤„ç†æ–‡ä»¶: {test_file.name}\n")
    
    # åˆ›å»ºchunker
    chunker = SemanticChunker(
        target_chunk_size=800,
        max_chunk_size=1500,
        min_chunk_size=200
    )
    
    # æ‰§è¡Œåˆ†å—
    chunks = chunker.chunk_markdown_file(test_file)
    
    print(f"âœ… æˆåŠŸç”Ÿæˆ {len(chunks)} ä¸ªchunks\n")
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = chunker.get_statistics(chunks)
    print("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  æ€»chunkæ•°: {stats['total_chunks']}")
    print(f"  å¹³å‡å¤§å°: {stats['avg_chunk_size']:.1f} å­—ç¬¦")
    print(f"  æœ€å°å¤§å°: {stats['min_chunk_size']} å­—ç¬¦")
    print(f"  æœ€å¤§å¤§å°: {stats['max_chunk_size']} å­—ç¬¦")
    print(f"  ç±»å‹åˆ†å¸ƒ: {stats['chunk_type_distribution']}")
    print(f"  åŒ…å«å›¾ç‰‡çš„chunkæ•°: {stats['chunks_with_images']}")
    print(f"  åŒ…å«è¡¨æ ¼çš„chunkæ•°: {stats['chunks_with_tables']}")
    print(f"  åŒ…å«åˆ—è¡¨çš„chunkæ•°: {stats['chunks_with_lists']}")
    
    # æ˜¾ç¤ºå‰3ä¸ªchunkçš„ç¤ºä¾‹
    print("\nğŸ“ å‰3ä¸ªchunkç¤ºä¾‹:")
    for i, chunk in enumerate(chunks[:3], 1):
        print(f"\n--- Chunk {i} ---")
        print(f"ID: {chunk.chunk_id}")
        print(f"ç±»å‹: {chunk.metadata.chunk_type}")
        print(f"ç« èŠ‚è·¯å¾„: {' > '.join(chunk.metadata.section_path)}")
        print(f"å­—ç¬¦æ•°: {chunk.metadata.char_count}")
        print(f"æ–‡æœ¬é¢„è§ˆ: {chunk.text[:100]}...")
        if chunk.metadata.image_refs:
            print(f"å›¾ç‰‡å¼•ç”¨: {chunk.metadata.image_refs}")
    
    # ä¿å­˜ç»“æœ
    output_file = project_root / "data/chunks/ä¿é™©åŸºç¡€çŸ¥å¤šå°‘_chunks.json"
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(
            [chunk.to_dict() for chunk in chunks],
            f,
            ensure_ascii=False,
            indent=2
        )
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    return chunks


def test_table_handling():
    """æµ‹è¯•è¡¨æ ¼å¤„ç†"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•2: è¡¨æ ¼å¤„ç†")
    print("=" * 80)
    
    # æµ‹è¯•åŒ…å«è¡¨æ ¼çš„æ–‡ä»¶
    test_file = project_root / "data/processed/å¹³å®‰-å¯¿é™©è¯´æ˜ä¹¦/å¹³å®‰-å¯¿é™©è¯´æ˜ä¹¦/hybrid_auto/å¹³å®‰-å¯¿é™©è¯´æ˜ä¹¦.md"
    
    if not test_file.exists():
        print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
        return
    
    print(f"ğŸ“„ å¤„ç†æ–‡ä»¶: {test_file.name}\n")
    
    chunker = SemanticChunker(
        target_chunk_size=800,
        max_chunk_size=1500,
        min_chunk_size=200
    )
    
    chunks = chunker.chunk_markdown_file(test_file)
    
    # æ‰¾å‡ºæ‰€æœ‰åŒ…å«è¡¨æ ¼çš„chunk
    table_chunks = [c for c in chunks if c.metadata.has_table]
    
    print(f"âœ… æ€»å…± {len(chunks)} ä¸ªchunks")
    print(f"ğŸ“Š å…¶ä¸­åŒ…å«è¡¨æ ¼çš„chunk: {len(table_chunks)} ä¸ª\n")
    
    # æ˜¾ç¤ºç¬¬ä¸€ä¸ªè¡¨æ ¼chunk
    if table_chunks:
        print("ğŸ“‹ ç¬¬ä¸€ä¸ªè¡¨æ ¼chunkç¤ºä¾‹:")
        chunk = table_chunks[0]
        print(f"ID: {chunk.chunk_id}")
        print(f"ç« èŠ‚è·¯å¾„: {' > '.join(chunk.metadata.section_path)}")
        print(f"å­—ç¬¦æ•°: {chunk.metadata.char_count}")
        print(f"æ–‡æœ¬é¢„è§ˆ:\n{chunk.text[:300]}...")
    
    return chunks


def test_batch_processing():
    """æµ‹è¯•æ‰¹é‡å¤„ç†"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•3: æ‰¹é‡å¤„ç†ç›®å½•")
    print("=" * 80)
    
    input_dir = project_root / "data/processed"
    output_dir = project_root / "data/chunks"
    
    print(f"ğŸ“ è¾“å…¥ç›®å½•: {input_dir}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}\n")
    
    chunker = SemanticChunker(
        target_chunk_size=800,
        max_chunk_size=1500,
        min_chunk_size=200
    )
    
    results = chunker.chunk_directory(input_dir, output_dir)
    
    print(f"\nâœ… æ‰¹é‡å¤„ç†å®Œæˆ!")
    print(f"ğŸ“Š æ€»å…±å¤„ç†äº† {len(results)} ä¸ªæ–‡ä»¶")
    
    # ç»Ÿè®¡æ€»chunkæ•°
    total_chunks = sum(len(chunks) for chunks in results.values())
    print(f"ğŸ“Š æ€»å…±ç”Ÿæˆäº† {total_chunks} ä¸ªchunks")
    
    return results


def test_section_hierarchy():
    """æµ‹è¯•ç« èŠ‚å±‚çº§ä¿ç•™"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•4: ç« èŠ‚å±‚çº§ä¿ç•™")
    print("=" * 80)
    
    test_file = project_root / "data/processed/ä¿é™©åŸºç¡€çŸ¥å¤šå°‘/ä¿é™©åŸºç¡€çŸ¥å¤šå°‘/hybrid_auto/ä¿é™©åŸºç¡€çŸ¥å¤šå°‘.md"
    
    if not test_file.exists():
        print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
        return
    
    chunker = SemanticChunker()
    chunks = chunker.chunk_markdown_file(test_file)
    
    print("ğŸ“š ç« èŠ‚å±‚çº§ç¤ºä¾‹ (å‰10ä¸ªchunk):\n")
    for i, chunk in enumerate(chunks[:10], 1):
        section = ' > '.join(chunk.metadata.section_path) if chunk.metadata.section_path else '(æ— ç« èŠ‚)'
        print(f"{i:2d}. [{chunk.metadata.chunk_type:10s}] {section}")
        print(f"    å­—ç¬¦æ•°: {chunk.metadata.char_count}, çº§åˆ«: {chunk.metadata.heading_level}")
    
    return chunks


def test_semantic_splitting():
    """æµ‹è¯•è¯­ä¹‰åˆ‡å‰²"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•5: è¯­ä¹‰åˆ‡å‰²")
    print("=" * 80)
    
    splitter = SemanticSplitter()
    
    test_text = """è¢«ä¿é™©äººå› æ„å¤–ä¼¤å®³å¯¼è‡´èº«æ•…çš„ï¼Œä¿é™©äººæŒ‰ç…§åˆåŒçº¦å®šç»™ä»˜ä¿é™©é‡‘ã€‚
ä½†è¢«ä¿é™©äººå› è‡ªæ€å¯¼è‡´èº«æ•…çš„ï¼Œä¿é™©äººä¸æ‰¿æ‹…ä¿é™©è´£ä»»ã€‚
åœ¨ä¿é™©æœŸé—´å†…ï¼Œå¦‚æœè¢«ä¿é™©äººå‘ç”Ÿé‡å¤§ç–¾ç—…ï¼Œä¿é™©äººå°†æŒ‰ç…§åˆåŒçº¦å®šç»™ä»˜ä¿é™©é‡‘ã€‚"""
    
    print("åŸå§‹æ–‡æœ¬:")
    print(test_text)
    print("\nè¯­ä¹‰åŸå­:")
    
    atoms = splitter.split_into_semantic_atoms(test_text)
    for i, atom in enumerate(atoms, 1):
        print(f"\nåŸå­ {i}:")
        print(f"  ç±»å‹: {atom.semantic_type}")
        print(f"  è§¦å‘è¯: {atom.trigger_words}")
        print(f"  æ–‡æœ¬: {atom.text[:100]}...")


def test_terminology():
    """æµ‹è¯•æœ¯è¯­æå–"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•6: æœ¯è¯­æå–")
    print("=" * 80)
    
    terminology = InsuranceTerminology()
    
    test_text = """æœ¬ä¿é™©åˆåŒçº¦å®šçš„ä¿é™©è´£ä»»åŒ…æ‹¬ä»¥ä¸‹å†…å®¹ã€‚è¢«ä¿é™©äººå› æ„å¤–ä¼¤å®³å¯¼è‡´èº«æ•…çš„ï¼Œä¿é™©äººæŒ‰ç…§åˆåŒçº¦å®šç»™ä»˜ä¿é™©é‡‘ã€‚
ä¿é™©è´¹åº”åœ¨ä¿é™©æœŸé—´å†…æŒ‰æ—¶ç¼´çº³ã€‚ä¿é™©é‡‘é¢ä¸ºäººæ°‘å¸100ä¸‡å…ƒã€‚"""
    
    print("åŸå§‹æ–‡æœ¬:")
    print(test_text)
    print("\næå–çš„æœ¯è¯­:")
    
    terms = terminology.extract_terms(test_text)
    for term in sorted(terms):
        print(f"  - {term}")


def test_semantic_chunker_integration():
    """æµ‹è¯•è¯­ä¹‰åˆ‡å‰²å’Œæœ¯è¯­æå–çš„chunkeré›†æˆ"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•7: è¯­ä¹‰åˆ‡å‰²å’Œæœ¯è¯­æå–é›†æˆ")
    print("=" * 80)
    
    # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
    test_file = project_root / "data/test_semantic.md"
    test_content = """# ä¿é™©è´£ä»»

è¢«ä¿é™©äººå› æ„å¤–ä¼¤å®³å¯¼è‡´èº«æ•…çš„ï¼Œä¿é™©äººæŒ‰ç…§åˆåŒçº¦å®šç»™ä»˜ä¿é™©é‡‘ã€‚

ä½†è¢«ä¿é™©äººå› è‡ªæ€å¯¼è‡´èº«æ•…çš„ï¼Œä¿é™©äººä¸æ‰¿æ‹…ä¿é™©è´£ä»»ã€‚

åœ¨ä¿é™©æœŸé—´å†…ï¼Œå¦‚æœè¢«ä¿é™©äººå‘ç”Ÿé‡å¤§ç–¾ç—…ï¼Œä¿é™©äººå°†æŒ‰ç…§åˆåŒçº¦å®šç»™ä»˜ä¿é™©é‡‘ã€‚

ä¿é™©è´¹åº”åœ¨ä¿é™©æœŸé—´å†…æŒ‰æ—¶ç¼´çº³ã€‚ä¿é™©é‡‘é¢ä¸ºäººæ°‘å¸100ä¸‡å…ƒã€‚"""
    
    test_file.parent.mkdir(parents=True, exist_ok=True)
    with open(test_file, 'w', encoding='utf-8') as f:
        f.write(test_content)
    
    print(f"åˆ›å»ºæµ‹è¯•æ–‡ä»¶: {test_file}\n")
    
    # ä½¿ç”¨chunkerå¤„ç†ï¼ˆå¯ç”¨è¯­ä¹‰åˆ‡å‰²å’Œæœ¯è¯­æå–ï¼‰
    chunker = SemanticChunker(
        target_chunk_size=500,
        max_chunk_size=1000,
        min_chunk_size=100,
        enable_text_cleaning=True,
        enable_semantic_splitting=True,
        enable_terminology=True
    )
    
    chunks = chunker.chunk_markdown_file(test_file)
    
    print(f"ç”Ÿæˆäº† {len(chunks)} ä¸ªchunks\n")
    
    for i, chunk in enumerate(chunks, 1):
        print(f"--- Chunk {i} ---")
        print(f"è¯­ä¹‰ç±»å‹: {chunk.metadata.semantic_type}")
        print(f"è§¦å‘è¯: {chunk.metadata.trigger_words}")
        print(f"æ ¸å¿ƒæ¡æ¬¾åŒº: {chunk.metadata.is_core_section}")
        print(f"æ¡æ¬¾ç¼–å·: {chunk.metadata.clause_number}")
        print(f"æœ¯è¯­: {chunk.metadata.key_terms}")
        print(f"æ–‡æœ¬é•¿åº¦: {len(chunk.text)} å­—ç¬¦")
        print(f"æ–‡æœ¬é¢„è§ˆ: {chunk.text[:100]}...")
        print()
    
    # ä¿å­˜ç»“æœåˆ°data/chunks
    output_file = project_root / "data/chunks/test_semantic_chunks.json"
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(
            [chunk.to_dict() for chunk in chunks],
            f,
            ensure_ascii=False,
            indent=2
        )
    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    # æ¸…ç†æµ‹è¯•æ–‡ä»¶
    test_file.unlink()
    
    return chunks


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("\nğŸš€ å¼€å§‹æµ‹è¯• SemanticChunker\n")
    
    try:
        # æµ‹è¯•1: å•ä¸ªæ–‡ä»¶
        chunks1 = test_single_file()
        
        # æµ‹è¯•2: è¡¨æ ¼å¤„ç†
        chunks2 = test_table_handling()
        
        # æµ‹è¯•3: æ‰¹é‡å¤„ç†
        results = test_batch_processing()
        
        # æµ‹è¯•4: ç« èŠ‚å±‚çº§
        chunks4 = test_section_hierarchy()
        
        # æµ‹è¯•5: è¯­ä¹‰åˆ‡å‰²
        test_semantic_splitting()
        
        # æµ‹è¯•6: æœ¯è¯­æå–
        test_terminology()
        
        # æµ‹è¯•7: è¯­ä¹‰åˆ‡å‰²å’Œæœ¯è¯­æå–é›†æˆ
        chunks7 = test_semantic_chunker_integration()
        
        print("\n" + "=" * 80)
        print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
        print("=" * 80)
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
