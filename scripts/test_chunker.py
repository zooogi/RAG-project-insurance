#!/usr/bin/env python3
# scripts/test_chunker.py

"""
æµ‹è¯•chunkeråŠŸèƒ½çš„è„šæœ¬
"""

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from app.chunker import SemanticChunker
import json


def test_single_file():
    """æµ‹è¯•å•ä¸ªæ–‡ä»¶çš„åˆ†å—"""
    print("=" * 80)
    print("æµ‹è¯•1: å•ä¸ªæ–‡ä»¶åˆ†å—")
    print("=" * 80)
    
    # æµ‹è¯•æ–‡ä»¶è·¯å¾„
    test_file = project_root / "data/processed/ä¿é™©åŸºç¡€çŸ¥å¤šå°‘/ä¿é™©åŸºç¡€çŸ¥å¤šå°‘/hybrid_auto/ä¿é™©åŸºç¡€çŸ¥å¤šå°‘.md"
    
    if not test_file.exists():
        print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
        return
    
    print(f"ğŸ“„ å¤„ç†æ–‡ä»¶: {test_file.name}\n")
    
    # åˆ›å»ºchunker
    chunker = SemanticChunker(
        target_chunk_size=800,
        max_chunk_size=1500,
        min_chunk_size=200
    )
    
    # æ‰§è¡Œåˆ†å—
    chunks = chunker.chunk_markdown_file(test_file)
    
    print(f"âœ… æˆåŠŸç”Ÿæˆ {len(chunks)} ä¸ªchunks\n")
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = chunker.get_statistics(chunks)
    print("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  æ€»chunkæ•°: {stats['total_chunks']}")
    print(f"  å¹³å‡å¤§å°: {stats['avg_chunk_size']:.1f} å­—ç¬¦")
    print(f"  æœ€å°å¤§å°: {stats['min_chunk_size']} å­—ç¬¦")
    print(f"  æœ€å¤§å¤§å°: {stats['max_chunk_size']} å­—ç¬¦")
    print(f"  ç±»å‹åˆ†å¸ƒ: {stats['chunk_type_distribution']}")
    print(f"  åŒ…å«å›¾ç‰‡çš„chunkæ•°: {stats['chunks_with_images']}")
    print(f"  åŒ…å«è¡¨æ ¼çš„chunkæ•°: {stats['chunks_with_tables']}")
    print(f"  åŒ…å«åˆ—è¡¨çš„chunkæ•°: {stats['chunks_with_lists']}")
    
    # æ˜¾ç¤ºå‰3ä¸ªchunkçš„ç¤ºä¾‹
    print("\nğŸ“ å‰3ä¸ªchunkç¤ºä¾‹:")
    for i, chunk in enumerate(chunks[:3], 1):
        print(f"\n--- Chunk {i} ---")
        print(f"ID: {chunk.chunk_id}")
        print(f"ç±»å‹: {chunk.metadata.chunk_type}")
        print(f"ç« èŠ‚è·¯å¾„: {' > '.join(chunk.metadata.section_path)}")
        print(f"å­—ç¬¦æ•°: {chunk.metadata.char_count}")
        print(f"æ–‡æœ¬é¢„è§ˆ: {chunk.text[:100]}...")
        if chunk.metadata.image_refs:
            print(f"å›¾ç‰‡å¼•ç”¨: {chunk.metadata.image_refs}")
    
    # ä¿å­˜ç»“æœ
    output_file = project_root / "data/chunks/ä¿é™©åŸºç¡€çŸ¥å¤šå°‘_chunks.json"
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(
            [chunk.to_dict() for chunk in chunks],
            f,
            ensure_ascii=False,
            indent=2
        )
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    return chunks


def test_table_handling():
    """æµ‹è¯•è¡¨æ ¼å¤„ç†"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•2: è¡¨æ ¼å¤„ç†")
    print("=" * 80)
    
    # æµ‹è¯•åŒ…å«è¡¨æ ¼çš„æ–‡ä»¶
    test_file = project_root / "data/processed/å¹³å®‰-å¯¿é™©è¯´æ˜ä¹¦/å¹³å®‰-å¯¿é™©è¯´æ˜ä¹¦/hybrid_auto/å¹³å®‰-å¯¿é™©è¯´æ˜ä¹¦.md"
    
    if not test_file.exists():
        print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
        return
    
    print(f"ğŸ“„ å¤„ç†æ–‡ä»¶: {test_file.name}\n")
    
    chunker = SemanticChunker(
        target_chunk_size=800,
        max_chunk_size=1500,
        min_chunk_size=200
    )
    
    chunks = chunker.chunk_markdown_file(test_file)
    
    # æ‰¾å‡ºæ‰€æœ‰åŒ…å«è¡¨æ ¼çš„chunk
    table_chunks = [c for c in chunks if c.metadata.has_table]
    
    print(f"âœ… æ€»å…± {len(chunks)} ä¸ªchunks")
    print(f"ğŸ“Š å…¶ä¸­åŒ…å«è¡¨æ ¼çš„chunk: {len(table_chunks)} ä¸ª\n")
    
    # æ˜¾ç¤ºç¬¬ä¸€ä¸ªè¡¨æ ¼chunk
    if table_chunks:
        print("ğŸ“‹ ç¬¬ä¸€ä¸ªè¡¨æ ¼chunkç¤ºä¾‹:")
        chunk = table_chunks[0]
        print(f"ID: {chunk.chunk_id}")
        print(f"ç« èŠ‚è·¯å¾„: {' > '.join(chunk.metadata.section_path)}")
        print(f"å­—ç¬¦æ•°: {chunk.metadata.char_count}")
        print(f"æ–‡æœ¬é¢„è§ˆ:\n{chunk.text[:300]}...")
    
    return chunks


def test_batch_processing():
    """æµ‹è¯•æ‰¹é‡å¤„ç†"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•3: æ‰¹é‡å¤„ç†ç›®å½•")
    print("=" * 80)
    
    input_dir = project_root / "data/processed"
    output_dir = project_root / "data/chunks"
    
    print(f"ğŸ“ è¾“å…¥ç›®å½•: {input_dir}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}\n")
    
    chunker = SemanticChunker(
        target_chunk_size=800,
        max_chunk_size=1500,
        min_chunk_size=200
    )
    
    results = chunker.chunk_directory(input_dir, output_dir)
    
    print(f"\nâœ… æ‰¹é‡å¤„ç†å®Œæˆ!")
    print(f"ğŸ“Š æ€»å…±å¤„ç†äº† {len(results)} ä¸ªæ–‡ä»¶")
    
    # ç»Ÿè®¡æ€»chunkæ•°
    total_chunks = sum(len(chunks) for chunks in results.values())
    print(f"ğŸ“Š æ€»å…±ç”Ÿæˆäº† {total_chunks} ä¸ªchunks")
    
    return results


def test_section_hierarchy():
    """æµ‹è¯•ç« èŠ‚å±‚çº§ä¿ç•™"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•4: ç« èŠ‚å±‚çº§ä¿ç•™")
    print("=" * 80)
    
    test_file = project_root / "data/processed/ä¿é™©åŸºç¡€çŸ¥å¤šå°‘/ä¿é™©åŸºç¡€çŸ¥å¤šå°‘/hybrid_auto/ä¿é™©åŸºç¡€çŸ¥å¤šå°‘.md"
    
    if not test_file.exists():
        print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
        return
    
    chunker = SemanticChunker()
    chunks = chunker.chunk_markdown_file(test_file)
    
    print("ğŸ“š ç« èŠ‚å±‚çº§ç¤ºä¾‹ (å‰10ä¸ªchunk):\n")
    for i, chunk in enumerate(chunks[:10], 1):
        section = ' > '.join(chunk.metadata.section_path) if chunk.metadata.section_path else '(æ— ç« èŠ‚)'
        print(f"{i:2d}. [{chunk.metadata.chunk_type:10s}] {section}")
        print(f"    å­—ç¬¦æ•°: {chunk.metadata.char_count}, çº§åˆ«: {chunk.metadata.heading_level}")
    
    return chunks


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("\nğŸš€ å¼€å§‹æµ‹è¯• SemanticChunker\n")
    
    try:
        # æµ‹è¯•1: å•ä¸ªæ–‡ä»¶
        chunks1 = test_single_file()
        
        # æµ‹è¯•2: è¡¨æ ¼å¤„ç†
        chunks2 = test_table_handling()
        
        # æµ‹è¯•3: æ‰¹é‡å¤„ç†
        results = test_batch_processing()
        
        # æµ‹è¯•4: ç« èŠ‚å±‚çº§
        chunks4 = test_section_hierarchy()
        
        print("\n" + "=" * 80)
        print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
        print("=" * 80)
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
