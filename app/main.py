"""
RAG Pipeline ä¸»æµç¨‹æ¨¡å—
ä¸²è”OCR -> æ¸…æ´— -> Chunk -> Embedding -> Rerank -> LLMçš„å®Œæ•´æµç¨‹
"""
import json
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import numpy as np

from app.config import RAGPipelineConfig
from app.ocr import DocumentProcessor
from app.chunker import SemanticChunker, Chunk
from app.embedder import Embedder, create_embedder
from app.reranker import Reranker, create_reranker
from app.llm import LLM, create_llm


class RAGPipeline:
    """
    RAGå®Œæ•´æµç¨‹ç®¡é“
    
    åŠŸèƒ½ï¼š
    1. å¤„ç†æ–‡æ¡£ï¼šOCR -> æ¸…æ´— -> Chunk -> Embeddingç´¢å¼•æ„å»º
    2. æŸ¥è¯¢ç­”æ¡ˆï¼šQuery -> Rerankæ£€ç´¢ -> LLMç”Ÿæˆç­”æ¡ˆ
    """
    
    def __init__(self, config: Optional[RAGPipelineConfig] = None):
        """
        åˆå§‹åŒ–RAG Pipeline
        
        Args:
            config: é…ç½®å¯¹è±¡ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        """
        self.config = config or RAGPipelineConfig()
        
        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶ï¼ˆå»¶è¿ŸåŠ è½½ï¼Œåªåœ¨éœ€è¦æ—¶åˆ›å»ºï¼‰
        self.ocr_processor: Optional[DocumentProcessor] = None
        self.chunker: Optional[SemanticChunker] = None
        self.embedder: Optional[Embedder] = None
        self.reranker: Optional[Reranker] = None
        self.llm: Optional[LLM] = None
        
        # å­˜å‚¨å·²å¤„ç†çš„chunksï¼ˆç”¨äºæ„å»ºç´¢å¼•ï¼‰
        self.chunks: List[Chunk] = []
        self.index_built: bool = False
    
    # ==================== ç»„ä»¶åˆå§‹åŒ–æ–¹æ³• ====================
    
    def _init_ocr_processor(self) -> DocumentProcessor:
        """åˆå§‹åŒ–OCRå¤„ç†å™¨"""
        if self.ocr_processor is None:
            self.ocr_processor = DocumentProcessor(
                output_base_dir=str(self.config.ocr_output_dir),
                source=self.config.ocr_source,
                use_gpu=self.config.ocr_use_gpu,
                use_paddleocr_slim=self.config.ocr_use_paddleocr_slim
            )
        return self.ocr_processor
    
    def _init_chunker(self) -> SemanticChunker:
        """åˆå§‹åŒ–Chunker"""
        if self.chunker is None:
            from app.text_cleaner import TextCleaner
            
            text_cleaner = None
            if self.config.enable_text_cleaning:
                text_cleaner = TextCleaner(
                    min_repeat_length=self.config.text_cleaner_min_repeat_length,
                    repeat_threshold=self.config.text_cleaner_repeat_threshold
                )
            
            self.chunker = SemanticChunker(
                target_chunk_size=self.config.chunker_target_size,
                max_chunk_size=self.config.chunker_max_size,
                min_chunk_size=self.config.chunker_min_size,
                overlap_size=self.config.chunker_overlap_size,
                enable_text_cleaning=self.config.enable_text_cleaning,
                text_cleaner=text_cleaner,
                save_cleaned_text=self.config.save_cleaned_text,
                cleaned_output_dir=self.config.cleaned_dir,
                enable_semantic_splitting=self.config.enable_semantic_splitting,
                enable_terminology=self.config.enable_terminology,
                terminology_file=self.config.terminology_file
            )
        return self.chunker
    
    def _init_embedder(self) -> Embedder:
        """åˆå§‹åŒ–Embedder"""
        if self.embedder is None:
            self.embedder = create_embedder(
                model_name=self.config.embedder_model_name,
                model_path=self.config.embedder_model_path,
                device=self.config.embedder_device,
                use_mirror=self.config.embedder_use_mirror
            )
        return self.embedder
    
    def _init_reranker(self) -> Reranker:
        """åˆå§‹åŒ–Reranker"""
        if self.reranker is None:
            embedder = self._init_embedder()
            self.reranker = create_reranker(
                embedder=embedder,
                reranker_model_name=self.config.reranker_model_name,
                reranker_model_path=self.config.reranker_model_path,
                device=self.config.reranker_device,
                use_metadata=self.config.reranker_use_metadata,
                use_mirror=self.config.reranker_use_mirror
            )
        return self.reranker
    
    def _init_llm(self) -> LLM:
        """åˆå§‹åŒ–LLM"""
        if self.llm is None:
            self.llm = create_llm(
                model_name=self.config.llm_model_name,
                model_path=self.config.llm_model_path,
                device=self.config.llm_device,
                use_mirror=self.config.llm_use_mirror,
                max_new_tokens=self.config.llm_max_new_tokens,
                temperature=self.config.llm_temperature,
                top_p=self.config.llm_top_p,
                load_in_8bit=self.config.llm_load_in_8bit,
                load_in_4bit=self.config.llm_load_in_4bit
            )
        return self.llm
    
    # ==================== æ–‡æ¡£å¤„ç†æµç¨‹ ====================
    
    def process_documents(
        self,
        input_path: Optional[Path] = None,
        overwrite: bool = False
    ) -> Dict[str, Any]:
        """
        å¤„ç†æ–‡æ¡£ï¼šOCR -> æ¸…æ´— -> Chunk
        
        Args:
            input_path: è¾“å…¥è·¯å¾„ï¼ˆæ–‡ä»¶æˆ–ç›®å½•ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨configä¸­çš„raw_data_dir
            overwrite: æ˜¯å¦è¦†ç›–å·²å­˜åœ¨çš„å¤„ç†ç»“æœ
        
        Returns:
            å¤„ç†ç»“æœå­—å…¸ï¼ŒåŒ…å«ï¼š
            - ocr_results: OCRå¤„ç†ç»“æœåˆ—è¡¨
            - chunks: ç”Ÿæˆçš„chunksåˆ—è¡¨
            - chunk_files: ä¿å­˜çš„chunk JSONæ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        print("=" * 70)
        print("ğŸ“„ å¼€å§‹å¤„ç†æ–‡æ¡£")
        print("=" * 70)
        
        if input_path is None:
            input_path = self.config.raw_data_dir
        
        input_path = Path(input_path)
        
        # æ­¥éª¤1: OCRå¤„ç†
        print("\nã€æ­¥éª¤1/3ã€‘OCRå¤„ç†...")
        ocr_processor = self._init_ocr_processor()
        
        if input_path.is_file():
            # å¤„ç†å•ä¸ªæ–‡ä»¶
            ocr_result = ocr_processor.process_file(
                input_path,
                extract_images=self.config.ocr_extract_images,
                extract_tables=self.config.ocr_extract_tables,
                overwrite=overwrite
            )
            ocr_results = [ocr_result]
        else:
            # æ‰¹é‡å¤„ç†ç›®å½•
            ocr_results = ocr_processor.batch_process(
                input_path,
                overwrite=overwrite
            )
        
        print(f"âœ“ OCRå¤„ç†å®Œæˆï¼Œå…±å¤„ç† {len(ocr_results)} ä¸ªæ–‡ä»¶")
        
        # æ­¥éª¤2: Chunkå¤„ç†
        print("\nã€æ­¥éª¤2/3ã€‘æ–‡æœ¬æ¸…æ´—å’Œåˆ†å—...")
        chunker = self._init_chunker()
        
        all_chunks = []
        chunk_files = []
        
        # ä»OCRç»“æœä¸­æ‰¾åˆ°æ‰€æœ‰markdownæ–‡ä»¶
        md_files = []
        for ocr_result in ocr_results:
            if ocr_result.get('file_type') in ['pdf', 'image', 'csv']:
                # å°è¯•å¤šç§æ–¹å¼è·å–markdownæ–‡ä»¶è·¯å¾„
                md_path = None
                if 'files' in ocr_result and 'markdown' in ocr_result['files']:
                    md_path = Path(ocr_result['files']['markdown'])
                elif 'output_path' in ocr_result:
                    md_path = Path(ocr_result['output_path'])
                elif 'output_dir' in ocr_result:
                    # ä»output_dirå’Œfile_nameæ„å»ºè·¯å¾„
                    output_dir = Path(ocr_result['output_dir'])
                    file_name = ocr_result.get('file_name', '')
                    # PDFæ–‡ä»¶å¯èƒ½åœ¨å­ç›®å½•ä¸­
                    if ocr_result.get('file_type') == 'pdf':
                        md_path = output_dir / file_name / "hybrid_auto" / f"{file_name}.md"
                    else:
                        md_path = output_dir / f"{file_name}.md"
                
                if md_path and md_path.exists() and md_path.suffix == '.md':
                    md_files.append(md_path)
                elif md_path:
                    print(f"âš  æœªæ‰¾åˆ°markdownæ–‡ä»¶: {md_path}")
        
        # å¤„ç†æ¯ä¸ªmarkdownæ–‡ä»¶
        for md_file in md_files:
            print(f"\nå¤„ç†æ–‡ä»¶: {md_file.name}")
            chunks = chunker.chunk_markdown_file(md_file)
            all_chunks.extend(chunks)
            
            # ä¿å­˜chunksåˆ°JSONæ–‡ä»¶
            chunk_file = self.config.chunks_dir / f"{md_file.stem}_chunks.json"
            with open(chunk_file, 'w', encoding='utf-8') as f:
                json.dump(
                    [chunk.to_dict() for chunk in chunks],
                    f,
                    ensure_ascii=False,
                    indent=2
                )
            chunk_files.append(chunk_file)
            print(f"âœ“ ç”Ÿæˆ {len(chunks)} ä¸ªchunksï¼Œå·²ä¿å­˜åˆ°: {chunk_file}")
        
        print(f"\nâœ“ åˆ†å—å¤„ç†å®Œæˆï¼Œå…±ç”Ÿæˆ {len(all_chunks)} ä¸ªchunks")
        
        # æ­¥éª¤3: æ„å»ºEmbeddingç´¢å¼•
        print("\nã€æ­¥éª¤3/3ã€‘æ„å»ºEmbeddingç´¢å¼•...")
        if all_chunks:
            reranker = self._init_reranker()
            reranker.build_index(all_chunks)
            self.chunks = all_chunks
            self.index_built = True
            print(f"âœ“ ç´¢å¼•æ„å»ºå®Œæˆï¼Œå…± {len(all_chunks)} ä¸ªchunks")
        else:
            print("âš  æ²¡æœ‰chunkså¯æ„å»ºç´¢å¼•")
        
        print("\n" + "=" * 70)
        print("âœ“ æ–‡æ¡£å¤„ç†æµç¨‹å®Œæˆï¼")
        print("=" * 70)
        
        return {
            "ocr_results": ocr_results,
            "chunks": all_chunks,
            "chunk_files": chunk_files,
            "index_built": self.index_built
        }
    
    def load_chunks_from_files(
        self,
        chunk_files: Optional[List[Path]] = None
    ) -> List[Chunk]:
        """
        ä»JSONæ–‡ä»¶åŠ è½½chunkså¹¶æ„å»ºç´¢å¼•
        
        Args:
            chunk_files: chunk JSONæ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä»chunks_diråŠ è½½æ‰€æœ‰æ–‡ä»¶
        
        Returns:
            åŠ è½½çš„chunksåˆ—è¡¨
        """
        if chunk_files is None:
            # åŠ è½½chunks_dirä¸‹çš„æ‰€æœ‰chunkæ–‡ä»¶
            chunk_files = list(self.config.chunks_dir.glob("*_chunks.json"))
        
        print(f"ğŸ“‚ ä» {len(chunk_files)} ä¸ªæ–‡ä»¶åŠ è½½chunks...")
        
        all_chunks = []
        for chunk_file in chunk_files:
            with open(chunk_file, 'r', encoding='utf-8') as f:
                chunk_dicts = json.load(f)
            
            chunks = [Chunk.from_dict(chunk_dict) for chunk_dict in chunk_dicts]
            all_chunks.extend(chunks)
            print(f"âœ“ ä» {chunk_file.name} åŠ è½½äº† {len(chunks)} ä¸ªchunks")
        
        # æ„å»ºç´¢å¼•
        if all_chunks:
            reranker = self._init_reranker()
            reranker.build_index(all_chunks)
            self.chunks = all_chunks
            self.index_built = True
            print(f"\nâœ“ ç´¢å¼•æ„å»ºå®Œæˆï¼Œå…± {len(all_chunks)} ä¸ªchunks")
        
        return all_chunks
    
    # ==================== æŸ¥è¯¢æµç¨‹ ====================
    
    def query(
        self,
        query: str,
        use_rerank: bool = True,
        return_chunks: bool = False
    ) -> Dict[str, Any]:
        """
        æŸ¥è¯¢ç­”æ¡ˆï¼šæ£€ç´¢ -> Rerank -> LLMç”Ÿæˆ
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬
            use_rerank: æ˜¯å¦ä½¿ç”¨rerank
            return_chunks: æ˜¯å¦åœ¨ç»“æœä¸­è¿”å›æ£€ç´¢åˆ°çš„chunks
        
        Returns:
            ç»“æœå­—å…¸ï¼ŒåŒ…å«ï¼š
            - answer: LLMç”Ÿæˆçš„ç­”æ¡ˆ
            - query: åŸå§‹æŸ¥è¯¢
            - chunks: æ£€ç´¢åˆ°çš„chunksï¼ˆå¦‚æœreturn_chunks=Trueï¼‰
            - metadata: å…¶ä»–å…ƒä¿¡æ¯
        """
        if not self.index_built:
            raise ValueError("ç´¢å¼•æœªæ„å»ºï¼è¯·å…ˆè°ƒç”¨ process_documents() æˆ– load_chunks_from_files()")
        
        print("=" * 70)
        print(f"ğŸ” æŸ¥è¯¢: {query}")
        print("=" * 70)
        
        # æ­¥éª¤1: æ£€ç´¢
        print("\nã€æ­¥éª¤1/2ã€‘æ£€ç´¢ç›¸å…³chunks...")
        reranker = self._init_reranker()
        
        search_results = reranker.search(
            query,
            rank_top_k=self.config.reranker_rank_top_k,
            rerank_top_k=self.config.reranker_rerank_top_k,
            use_rerank=use_rerank
        )
        
        retrieved_chunks = [chunk for chunk, _, _ in search_results]
        print(f"âœ“ æ£€ç´¢åˆ° {len(retrieved_chunks)} ä¸ªç›¸å…³chunks")
        
        # æ­¥éª¤2: LLMç”Ÿæˆç­”æ¡ˆ
        print("\nã€æ­¥éª¤2/2ã€‘ç”Ÿæˆç­”æ¡ˆ...")
        llm = self._init_llm()
        
        result = llm.answer(
            query,
            retrieved_chunks,
            max_context_length=self.config.llm_max_context_length
        )
        
        print(f"âœ“ ç­”æ¡ˆç”Ÿæˆå®Œæˆ")
        
        # æ„å»ºè¿”å›ç»“æœ
        response = {
            "answer": result["answer"],
            "query": query,
            "num_chunks_used": result["num_chunks_used"],
            "prompt_length": result["prompt_length"],
            "metadata": {
                "rank_top_k": self.config.reranker_rank_top_k,
                "rerank_top_k": self.config.reranker_rerank_top_k,
                "use_rerank": use_rerank
            }
        }
        
        if return_chunks:
            response["chunks"] = [
                {
                    "chunk_id": chunk.chunk_id,
                    "text": chunk.text[:200] + "..." if len(chunk.text) > 200 else chunk.text,
                    "section_path": chunk.metadata.section_path,
                    "score": score,
                    "rank_score": info.get("rank_score"),
                    "rerank_score": info.get("rerank_score")
                }
                for chunk, score, info in search_results
            ]
        
        return response
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–Pipelineå½“å‰çŠ¶æ€"""
        return {
            "index_built": self.index_built,
            "num_chunks": len(self.chunks),
            "components_loaded": {
                "ocr_processor": self.ocr_processor is not None,
                "chunker": self.chunker is not None,
                "embedder": self.embedder is not None,
                "reranker": self.reranker is not None,
                "llm": self.llm is not None
            }
        }


# ==================== ä¾¿æ·å‡½æ•° ====================

def create_pipeline(config: Optional[RAGPipelineConfig] = None) -> RAGPipeline:
    """
    åˆ›å»ºRAG Pipelineå®ä¾‹çš„ä¾¿æ·å‡½æ•°
    
    Args:
        config: é…ç½®å¯¹è±¡ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
    
    Returns:
        RAGPipelineå®ä¾‹
    """
    return RAGPipeline(config)


# ==================== å‘½ä»¤è¡Œæ¥å£ ====================

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="RAG Pipeline - å®Œæ•´çš„RAGæµç¨‹")
    parser.add_argument(
        "mode",
        choices=["process", "query", "load"],
        help="è¿è¡Œæ¨¡å¼: process(å¤„ç†æ–‡æ¡£), query(æŸ¥è¯¢), load(åŠ è½½chunks)"
    )
    parser.add_argument(
        "--input",
        type=str,
        help="è¾“å…¥è·¯å¾„ï¼ˆæ–‡ä»¶æˆ–ç›®å½•ï¼Œç”¨äºprocessæ¨¡å¼ï¼‰"
    )
    parser.add_argument(
        "--query",
        type=str,
        help="æŸ¥è¯¢æ–‡æœ¬ï¼ˆç”¨äºqueryæ¨¡å¼ï¼‰"
    )
    parser.add_argument(
        "--chunks-dir",
        type=str,
        help="Chunksç›®å½•ï¼ˆç”¨äºloadæ¨¡å¼ï¼‰"
    )
    parser.add_argument(
        "--config",
        type=str,
        help="é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼Œå¯é€‰ï¼‰"
    )
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    config = RAGPipelineConfig()
    if args.config:
        import json
        with open(args.config, 'r', encoding='utf-8') as f:
            config_dict = json.load(f)
            # æ›´æ–°é…ç½®
            for key, value in config_dict.items():
                if hasattr(config, key):
                    setattr(config, key, value)
    
    # åˆ›å»ºpipeline
    pipeline = create_pipeline(config)
    
    if args.mode == "process":
        # å¤„ç†æ–‡æ¡£
        input_path = Path(args.input) if args.input else None
        result = pipeline.process_documents(input_path)
        print(f"\nâœ“ å¤„ç†å®Œæˆï¼å…±ç”Ÿæˆ {len(result['chunks'])} ä¸ªchunks")
    
    elif args.mode == "load":
        # åŠ è½½chunks
        if args.chunks_dir:
            config.chunks_dir = Path(args.chunks_dir)
        chunks = pipeline.load_chunks_from_files()
        print(f"\nâœ“ åŠ è½½å®Œæˆï¼å…± {len(chunks)} ä¸ªchunks")
    
    elif args.mode == "query":
        # æŸ¥è¯¢
        if not args.query:
            print("é”™è¯¯: queryæ¨¡å¼éœ€è¦æä¾› --query å‚æ•°")
            exit(1)
        
        # å¦‚æœç´¢å¼•æœªæ„å»ºï¼Œå°è¯•åŠ è½½chunks
        if not pipeline.index_built:
            print("ç´¢å¼•æœªæ„å»ºï¼Œå°è¯•åŠ è½½chunks...")
            pipeline.load_chunks_from_files()
        
        result = pipeline.query(args.query, return_chunks=True)
        print(f"\nç­”æ¡ˆ: {result['answer']}")
        print(f"\nä½¿ç”¨çš„chunksæ•°é‡: {result['num_chunks_used']}")
